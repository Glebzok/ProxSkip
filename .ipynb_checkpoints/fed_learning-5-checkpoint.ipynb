{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAzyeyeyivwH",
    "outputId": "e92cc6b7-ac7e-475b-ddf0-3c581bb2a314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/konstmish/opt_methods.git\n",
      "  Cloning https://github.com/konstmish/opt_methods.git to /private/var/folders/1r/dhtw1ntj2klfnlqmdcmdbtcm71p3bh/T/pip-req-build-s5eeo08u\n",
      "Requirement already satisfied (use --upgrade to upgrade): opt-methods==0.1.1 from git+https://github.com/konstmish/opt_methods.git in /opt/anaconda3/lib/python3.8/site-packages\n",
      "Building wheels for collected packages: opt-methods\n",
      "  Building wheel for opt-methods (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-methods: filename=opt_methods-0.1.1-py3-none-any.whl size=53038 sha256=4e5189f3b513b278f626288aa21bf4e078a8c8084c53625690ab20892e1a06d8\n",
      "  Stored in directory: /private/var/folders/1r/dhtw1ntj2klfnlqmdcmdbtcm71p3bh/T/pip-ephem-wheel-cache-21sk_jyt/wheels/bf/b4/48/3c91e0f1794f4eeba87acece649fcbd94f4b9a93674874633b\n",
      "Successfully built opt-methods\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/konstmish/opt_methods.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEcNMoYujO_g",
    "outputId": "a22942d0-45ae-49b8-eaad-6da6f5b0cc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ray in /opt/anaconda3/lib/python3.8/site-packages (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=7.0 in /opt/anaconda3/lib/python3.8/site-packages (from ray) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: msgpack<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from ray) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /opt/anaconda3/lib/python3.8/site-packages (from ray) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/anaconda3/lib/python3.8/site-packages (from ray) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: aiosignal in /opt/anaconda3/lib/python3.8/site-packages (from ray) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: frozenlist in /opt/anaconda3/lib/python3.8/site-packages (from ray) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.15.3 in /opt/anaconda3/lib/python3.8/site-packages (from ray) (3.18.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/anaconda3/lib/python3.8/site-packages (from ray) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16; python_version < \"3.9\" in /opt/anaconda3/lib/python3.8/site-packages (from ray) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: attrs in /opt/anaconda3/lib/python3.8/site-packages (from ray) (20.3.0)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv in /opt/anaconda3/lib/python3.8/site-packages (from ray) (20.14.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<=1.43.0,>=1.28.1 in /opt/anaconda3/lib/python3.8/site-packages (from ray) (1.43.0)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema in /opt/anaconda3/lib/python3.8/site-packages (from ray) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->ray) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->ray) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->ray) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->ray) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: six<2,>=1.9.0 in /opt/anaconda3/lib/python3.8/site-packages (from virtualenv->ray) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: distlib<1,>=0.3.1 in /opt/anaconda3/lib/python3.8/site-packages (from virtualenv->ray) (0.3.4)\n",
      "Requirement already satisfied, skipping upgrade: platformdirs<3,>=2 in /opt/anaconda3/lib/python3.8/site-packages (from virtualenv->ray) (2.5.2)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema->ray) (0.17.3)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from jsonschema->ray) (50.3.1.post20201107)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D_ulQQc9jIrx"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import psutil\n",
    "import ray\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "from sklearn.datasets import load_svmlight_file, fetch_rcv1\n",
    "\n",
    "from optmethods.datasets import get_dataset\n",
    "from optmethods.first_order import Adgd, Gd, Nesterov, RestNest\n",
    "from optmethods.loss import LogisticRegression\n",
    "from optmethods.utils import get_trace, relative_round\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", font_scale=1.2, palette=sns.color_palette(\"bright\"), color_codes=False)\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = 'DejaVu Sans'\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams['figure.figsize'] = (9, 6)\n",
    "# Multiple plots in one figure: code below\n",
    "# import matplotlib.backends.backend_pdf as bf\n",
    "# <code for plt>\n",
    "# pdf = bf.PdfPages(\"./output.pdf\")\n",
    "# for fig in xrange(1, plt.figure().number):\n",
    "#         pdf.savefig( fig )\n",
    "# pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aN_WVLzJi3vP"
   },
   "outputs": [],
   "source": [
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "# ray.init(num_cpus=num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eentqz5FkZtV"
   },
   "source": [
    "## Get data and define problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3hzL7NKAi4RH",
    "outputId": "434154f8-14f1-494e-bb9a-08696b6b8b01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the smoothness constant via SVD, it may take a few minutes...\n",
      "0.6611558746661748 0.0006611558746661747\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "import urllib.request\n",
    "w8a_url = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/w8a\"\n",
    "a9a_url = \"https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a9a\"\n",
    "data_path = './w8a'\n",
    "f = urllib.request.urlretrieve(w8a_url, data_path)\n",
    "# f = urllib.request.urlretrieve(a9a_url, data_path)\n",
    "A, b = sklearn.datasets.load_svmlight_file(data_path)\n",
    "\n",
    "n, dim = A.shape\n",
    "if n % num_cpus != 0:\n",
    "    A = A[:n - (n % num_cpus)]\n",
    "    b = b[:n - (n % num_cpus)]\n",
    "b_unique = np.unique(b)\n",
    "if (b_unique == [1, 2]).all():\n",
    "    # Transform labels {1, 2} to {0, 1}\n",
    "    b = b - 1\n",
    "elif (b_unique == [-1, 1]).all():\n",
    "    # Transform labels {-1, 1} to {0, 1}\n",
    "    b = (b+1) / 2\n",
    "else:\n",
    "    # replace class labels with 0's and 1's\n",
    "    b = 1. * (b == b[0])\n",
    "# A = A.toarray()\n",
    "l1 = 0\n",
    "loss = LogisticRegression(A, b, l1=l1, l2=0)\n",
    "n, dim = A.shape\n",
    "if n <= 20000 or dim <= 20000:\n",
    "    print('Computing the smoothness constant via SVD, it may take a few minutes...')\n",
    "L = loss.smoothness\n",
    "# l2 = 0\n",
    "l2 = 1e-3 * L\n",
    "loss.l2 = l2\n",
    "# x0 = csc_matrix((dim, 1))\n",
    "x0 = np.zeros(dim)\n",
    "n_epoch = 100\n",
    "# it_max = (n_epoch * n) // batch_size\n",
    "trace_len = 300\n",
    "\n",
    "print(L, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebqm_RYbkejf"
   },
   "source": [
    "## Solve problem by Nesterov's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "7QxXgy56i4vq",
    "outputId": "7060dae0-c0cb-4ffc-de99-f84bf7ab0fda"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rest = RestNest(loss=loss, doubling=True)\n",
    "rest_tr = rest.run(x0=x0, it_max=1000)\n",
    "rest_tr.compute_loss_of_iterates()\n",
    "rest.trace.plot_losses()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teevtNIPkPPy"
   },
   "source": [
    "## Define local methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O78aFoMkUR-"
   },
   "source": [
    "### Local SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "l3m85O7TkNwN"
   },
   "outputs": [],
   "source": [
    "from optmethods.optimizer import StochasticOptimizer\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(self, shuffle=True, prox_skip=False, loss=None, it_local=None, batch_size=1):\n",
    "        self.loss = loss\n",
    "        self.shuffle = shuffle\n",
    "        self.prox_skip = prox_skip\n",
    "        self.it_local = it_local\n",
    "        self.batch_size = batch_size\n",
    "        self.c = None\n",
    "        self.h = None\n",
    "        self.rng_skip = np.random.default_rng(42) # random number generator for random synchronizations\n",
    "    \n",
    "    def run_local(self, x, lr):\n",
    "        self.x = x * 1.\n",
    "        if self.shuffle:\n",
    "            self.run_local_shuffle(lr)\n",
    "        elif self.prox_skip:\n",
    "            print(f\"running prox skip with x={x}\")\n",
    "            self.run_prox_skip(lr)\n",
    "        else:\n",
    "            self.run_local_sgd(lr)\n",
    "        return self.x\n",
    "    \n",
    "    def run_local_shuffle(self, lr):\n",
    "        permutation = np.random.permutation(self.loss.n)\n",
    "        i = 0\n",
    "        while i < self.loss.n:\n",
    "            i_max = min(self.loss.n, i + self.batch_size)\n",
    "            idx = permutation[i:i_max]\n",
    "            self.x -= lr * self.loss.stochastic_gradient(self.x, idx=idx)\n",
    "            i += self.batch_size\n",
    "    \n",
    "    def run_local_sgd(self, lr):\n",
    "        for i in range(self.it_local):\n",
    "            if self.batch_size is None:\n",
    "                self.x -= lr * self.loss.gradient(self.x)\n",
    "            else:\n",
    "                self.x -= lr * self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "    \n",
    "    def run_scaffold(self, x, lr, c):\n",
    "        # as in the original scaffold paper, we use their Option II\n",
    "        self.x = x * 1.\n",
    "        if self.c is None:\n",
    "            self.c = self.x * 0. #initialize zero vector of the same dimension\n",
    "        for i in range(self.it_local):\n",
    "            if self.batch_size is None:\n",
    "                g = self.loss.gradient(self.x)\n",
    "            else:\n",
    "                g = self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "            self.x -= lr * (g - self.c + c)\n",
    "        self.c += 1 / (self.it_local * lr) * (x - self.x) - c\n",
    "        return self.x\n",
    "\n",
    "    def run_prox_skip(self, lr):\n",
    "        p = 1 / self.it_local\n",
    "        if self.h is None:\n",
    "            # first iteration\n",
    "            self.h = self.x * 0. # initialize zero vector of the same dimension\n",
    "        else:\n",
    "            # update the gradient estimate \n",
    "            self.h += p / self.lr * (self.x - self.x_before_averaing)\n",
    "        it_local = self.rng_skip.geometric(p=p) # since all workers use the same random seed, this number is the same for all of them\n",
    "\n",
    "        for i in range(it_local):\n",
    "            if self.batch_size is None:\n",
    "                g = self.loss.gradient(self.x)\n",
    "            else:\n",
    "                g = self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "            self.x -= lr * (g - self.h)\n",
    "        self.x_before_averaing = self.x * 1.\n",
    "    \n",
    "    def get_control_var(self):\n",
    "        return self.c\n",
    "\n",
    "\n",
    "class LocalSgd(StochasticOptimizer):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent with decreasing or constant learning rate.\n",
    "    \n",
    "    Arguments:\n",
    "        lr (float, optional): an estimate of the inverse smoothness constant\n",
    "        lr_decay_coef (float, optional): the coefficient in front of the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value\n",
    "            is mu/2, where mu is the strong convexity constant\n",
    "        lr_decay_power (float, optional): the power to exponentiate the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value is 1 (default: 1)\n",
    "        it_start_decay (int, optional): how many iterations the step-size is kept constant\n",
    "            By default, will be set to have about 2.5% of iterations with the step-size equal to lr0\n",
    "        batch_size (int, optional): the number of samples from the function to be used at each iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, it_local, n_workers=None, cohort_size=None, iid=False, lr0=None, lr_max=np.inf, lr_decay_coef=0,\n",
    "                 lr_decay_power=1, it_start_decay=None, batch_size=1, losses=None, *args, **kwargs):\n",
    "        super(LocalSgd, self).__init__(*args, **kwargs)\n",
    "        self.it_local = it_local\n",
    "        if n_workers is None:\n",
    "            n_workers = psutil.cpu_count(logical=False)\n",
    "        if cohort_size is None:\n",
    "            cohort_size = n_workers\n",
    "        self.n_workers = n_workers\n",
    "        self.cohort_size = cohort_size\n",
    "        self.iid = iid\n",
    "        self.lr0 = lr0\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_decay_coef = lr_decay_coef\n",
    "        self.lr_decay_power = lr_decay_power\n",
    "        self.it_start_decay = it_start_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.losses = losses\n",
    "        \n",
    "    def step(self):\n",
    "        denom_const = 1 / self.lr0\n",
    "        lr_decayed = 1 / (denom_const + self.it_local*self.lr_decay_coef*max(0, self.it-self.it_start_decay)**self.lr_decay_power)\n",
    "        if lr_decayed < 0:\n",
    "            lr_decayed = np.inf\n",
    "        self.lr = min(lr_decayed, self.lr_max)\n",
    "        x_id = ray.put(self.x)\n",
    "        \n",
    "        if self.cohort_size == self.n_workers:\n",
    "            self.x = np.mean(ray.get([worker.run_local.remote(x_id, self.lr) for worker in self.workers]), axis=0)\n",
    "        else:\n",
    "            cohort = np.random.choice(self.n_workers, self.cohort_size, replace=False)\n",
    "            self.x = np.mean(ray.get([self.workers[i].run_local.remote(x_id, self.lr) for i in cohort]), axis=0)\n",
    "    \n",
    "    def init_run(self, *args, **kwargs):\n",
    "        super(LocalSgd, self).init_run(*args, **kwargs)\n",
    "        if self.it_start_decay is None and np.isfinite(self.it_max):\n",
    "            self.it_start_decay = self.it_max // 40 if np.isfinite(self.it_max) else 0\n",
    "        if self.lr0 is None:\n",
    "            self.lr0 = 1 / self.loss.batch_smoothness(batch_size)\n",
    "        if self.iid:\n",
    "            loss_id = ray.put(self.loss)\n",
    "            self.workers = [Worker.remote(shuffle=False, loss=loss_id, it_local=self.it_local, batch_size=self.batch_size) for _ in range(self.n_workers)]\n",
    "        else:\n",
    "            loss_ids = [ray.put(self.losses[i]) for i in range(self.n_workers)]\n",
    "            self.workers = [Worker.remote(shuffle=False, loss=loss, it_local=self.it_local, batch_size=self.batch_size) for loss in loss_ids]\n",
    "        \n",
    "    def update_trace(self, first_iterations=10):\n",
    "        super(LocalSgd, self).update_trace()\n",
    "        \n",
    "    def terminate_workers(self):\n",
    "        for worker in self.workers:\n",
    "            ray.kill(worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgbaUWC7lN1u"
   },
   "source": [
    "### FedRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hQTuEMdmlKm1"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LocalShuffling(StochasticOptimizer):\n",
    "    \"\"\"\n",
    "    Shuffling-based stochastic gradient descent with decreasing or constant learning rate.\n",
    "    For a formal description and convergence guarantees, see\n",
    "        https://arxiv.org/abs/2006.05988\n",
    "    \n",
    "    The method is sensitive to finishing the final epoch, so it will terminate earlier \n",
    "    than it_max if it_max is not divisible by the number of steps per epoch.\n",
    "    \n",
    "    Arguments:\n",
    "        reshuffle (bool, optional): whether to get a new permuation for every new epoch.\n",
    "            For convex problems, only a single permutation should suffice and it can run faster (default: False)\n",
    "        prox_every_it (bool, optional): whether to use proximal operation every iteration \n",
    "            or only at the end of an epoch. Theory supports the latter. Only used if the loss includes\n",
    "            a proximal regularizer (default: False)\n",
    "        lr0 (float, optional): an estimate of the inverse smoothness constant, this step-size\n",
    "            is used for the first epoch_start_decay epochs. If not given, it will be set\n",
    "            with the value in the loss.\n",
    "        lr_max (float, optional): a maximal step-size never to be exceeded (default: np.inf)\n",
    "        lr_decay_coef (float, optional): the coefficient in front of the number of finished epochs\n",
    "            in the denominator of step-size. For strongly convex problems, a good value\n",
    "            is mu/3, where mu is the strong convexity constant\n",
    "        lr_decay_power (float, optional): the power to exponentiate the number of finished epochs\n",
    "            in the denominator of step-size. For strongly convex problems, a good value is 1 (default: 1)\n",
    "        epoch_start_decay (int, optional): how many epochs the step-size is kept constant\n",
    "            By default, will be set to have about 2.5% of iterations with the step-size equal to lr0\n",
    "        batch_size (int, optional): the number of samples from the function to be used at each iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, n_workers=None, iid=False, reshuffle=False, lr0=None, lr_max=np.inf, lr_decay_coef=0,\n",
    "                 lr_decay_power=1, epoch_start_decay=1, batch_size=1, \n",
    "                 losses=None, jumping=0, cohort_size=None, *args, **kwargs):\n",
    "        super(LocalShuffling, self).__init__(*args, **kwargs)\n",
    "        if n_workers is None:\n",
    "            n_workers = psutil.cpu_count(logical=False)\n",
    "        if cohort_size is None:\n",
    "            cohort_size = n_workers\n",
    "        self.n_workers = n_workers\n",
    "        self.cohort_size = cohort_size\n",
    "        self.iid = iid\n",
    "        self.reshuffle = reshuffle\n",
    "        self.lr0 = lr0\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_decay_coef = lr_decay_coef\n",
    "        self.lr_decay_power = lr_decay_power\n",
    "        self.epoch_start_decay = epoch_start_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.losses = losses\n",
    "        self.jumping = jumping\n",
    "        \n",
    "        if self.iid:\n",
    "            self.steps_per_epoch = math.ceil(self.loss.n/batch_size)\n",
    "        else:\n",
    "            self.steps_per_epoch = math.ceil(self.loss.n/batch_size/n_workers)\n",
    "        if epoch_start_decay is None and np.isfinite(self.epoch_max):\n",
    "            self.epoch_start_decay = 1 + self.epoch_max // 40\n",
    "        elif epoch_start_decay is None:\n",
    "            self.epoch_start_decay = 1\n",
    "        \n",
    "    def step(self):\n",
    "        if self.it%self.steps_per_epoch == 0 and self.reshuffle:\n",
    "            # Start new epoch\n",
    "            self.permutation = np.random.permutation(self.loss.n)\n",
    "            self.i = 0\n",
    "            self.sampled_permutations += 1\n",
    "        idx_perm = np.arange(self.i, min(self.loss.n, self.i+self.batch_size))\n",
    "        idx = self.permutation[idx_perm]\n",
    "        self.i += self.batch_size\n",
    "        # since the objective is 1/n sum_{i=1}^n f_i(x) + l2/2*||x||^2\n",
    "        # any incomplete minibatch should be normalized by batch_size\n",
    "        normalization = self.loss.n / self.steps_per_epoch\n",
    "        self.grad = self.loss.stochastic_gradient(self.x, idx=idx, normalization=normalization)\n",
    "        denom_const = 1 / self.lr0\n",
    "        lr_decayed = 1 / (denom_const + self.loss.n/self.batch_size*self.lr_decay_coef*max(0, self.finished_epochs-self.epoch_start_decay)**self.lr_decay_power)\n",
    "        self.lr = min(lr_decayed, self.lr_max)\n",
    "        x_id = ray.put(self.x)\n",
    "        if self.cohort_size == self.n_workers:\n",
    "            x_new = np.mean(ray.get([worker.run_local.remote(x_id, self.lr) for worker in self.workers]), axis=0)\n",
    "        else:\n",
    "            cohort = np.random.choice(self.n_workers, self.cohort_size, replace=False)\n",
    "            x_new = np.mean(ray.get([self.workers[i].run_local.remote(x_id, self.lr) for i in cohort]), axis=0)\n",
    "        if self.jumping:\n",
    "            full_grad_estim = (self.x - x_new) / self.lr\n",
    "            self.gd_lr = self.lr * (1 + self.jumping)\n",
    "            self.x = self.x - self.gd_lr * full_grad_estim\n",
    "            self.jumping_sum += self.jumping_coef\n",
    "        else:\n",
    "            self.x = x_new\n",
    "        self.finished_epochs += 1\n",
    "    \n",
    "    def init_run(self, *args, **kwargs):\n",
    "        super(LocalShuffling, self).init_run(*args, **kwargs)\n",
    "        if self.lr0 is None:\n",
    "            self.lr0 = 1 / self.loss.batch_smoothness(batch_size)\n",
    "        self.finished_epochs = 0\n",
    "        self.permutation = np.random.permutation(self.loss.n)\n",
    "        self.sampled_permutations = 1\n",
    "        self.epoch_max = self.it_max // self.steps_per_epoch\n",
    "        if self.iid:\n",
    "            loss_id = ray.put(self.loss)\n",
    "            self.workers = [Worker.remote(shuffle=True, loss=loss_id, it_local=None, batch_size=self.batch_size) for _ in range(self.n_workers)]\n",
    "        else:\n",
    "            loss_ids = [ray.put(self.losses[i]) for i in range(self.n_workers)]\n",
    "            self.workers = [Worker.remote(shuffle=True, loss=loss, it_local=None, batch_size=self.batch_size) for loss in loss_ids]\n",
    "        self.jumping_sum = 0\n",
    "        self.gd_lr = None\n",
    "        \n",
    "    def terminate_workers(self):\n",
    "        for worker in self.workers:\n",
    "            ray.kill(worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4gAjNNSpdf5"
   },
   "source": [
    "### Scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hKtE5dTYpfW1"
   },
   "outputs": [],
   "source": [
    "class Scaffold(StochasticOptimizer):\n",
    "    \"\"\"\n",
    "    Scaffold (local SGD with variance control).\n",
    "    \n",
    "    Arguments:\n",
    "        lr (float, optional): an estimate of the inverse smoothness constant\n",
    "        lr_decay_coef (float, optional): the coefficient in front of the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value\n",
    "            is mu/2, where mu is the strong convexity constant\n",
    "        lr_decay_power (float, optional): the power to exponentiate the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value is 1 (default: 1)\n",
    "        it_start_decay (int, optional): how many iterations the step-size is kept constant\n",
    "            By default, will be set to have about 2.5% of iterations with the step-size equal to lr0\n",
    "        batch_size (int, optional): the number of samples from the function to be used at each iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, it_local, n_workers=None, iid=False, lr0=None, lr_max=np.inf, \n",
    "                 lr_decay_coef=0, lr_decay_power=1, it_start_decay=None,\n",
    "                 batch_size=1, losses=None, global_lr=1., cohort_size=None, *args, **kwargs):\n",
    "        super(Scaffold, self).__init__(*args, **kwargs)\n",
    "        self.it_local = it_local\n",
    "        if n_workers is None:\n",
    "            n_workers = psutil.cpu_count(logical=False)\n",
    "        if cohort_size is None:\n",
    "            cohort_size = n_workers\n",
    "        self.n_workers = n_workers\n",
    "        self.cohort_size = cohort_size\n",
    "        self.iid = iid\n",
    "        self.lr0 = lr0\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_decay_coef = lr_decay_coef\n",
    "        self.lr_decay_power = lr_decay_power\n",
    "        self.it_start_decay = it_start_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.losses = losses\n",
    "        self.global_lr = global_lr\n",
    "        \n",
    "    def step(self):\n",
    "        denom_const = 1 / self.lr0\n",
    "        lr_decayed = 1 / (denom_const + self.it_local*self.lr_decay_coef*max(0, self.it-self.it_start_decay)**self.lr_decay_power)\n",
    "        if lr_decayed < 0:\n",
    "            lr_decayed = np.inf \n",
    "        self.lr = min(lr_decayed, self.lr_max)\n",
    "        x_id = ray.put(self.x)\n",
    "        c_id = ray.put(self.c)\n",
    "        if self.cohort_size == self.n_workers:\n",
    "            x_new = np.mean(ray.get([worker.run_scaffold.remote(x_id, self.lr, c_id) for worker in self.workers]), axis=0)\n",
    "            c_new = np.mean(ray.get([worker.get_control_var.remote() for worker in self.workers]), axis=0)\n",
    "        else:\n",
    "            cohort = np.random.choice(self.n_workers, self.cohort_size, replace=False)\n",
    "            x_new = np.mean(ray.get([self.workers[i].run_scaffold.remote(x_id, self.lr, c_id) for i in cohort]), axis=0)\n",
    "            c_new = np.mean(ray.get([self.workers[i].get_control_var.remote() for i in cohort]), axis=0)\n",
    "        if self.global_lr == 1:\n",
    "            self.x = x_new\n",
    "        else:\n",
    "            self.x += self.global_lr * (x_new - self.x)\n",
    "        self.c += self.cohort_size / self.n_workers * (c_new - self.c)\n",
    "    \n",
    "    def init_run(self, *args, **kwargs):\n",
    "        super(Scaffold, self).init_run(*args, **kwargs)\n",
    "        if self.it_start_decay is None and np.isfinite(self.it_max):\n",
    "            self.it_start_decay = self.it_max // 40 if np.isfinite(self.it_max) else 0\n",
    "        self.c = self.x * 0\n",
    "        if self.iid:\n",
    "            loss_id = ray.put(self.loss)\n",
    "            self.workers = [Worker.remote(shuffle=False, loss=loss_id, it_local=self.it_local, batch_size=self.batch_size) for _ in range(self.n_workers)]\n",
    "        else:\n",
    "            loss_ids = [ray.put(self.losses[i]) for i in range(self.n_workers)]\n",
    "            self.workers = [Worker.remote(shuffle=False, loss=loss, it_local=self.it_local, batch_size=self.batch_size) for loss in loss_ids]\n",
    "        \n",
    "    def terminate_workers(self):\n",
    "        for worker in self.workers:\n",
    "            ray.kill(worker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SUSgZuEkEYX"
   },
   "source": [
    "## FedLin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vIAyzBQ-kDx0"
   },
   "outputs": [],
   "source": [
    "class Fedlin(StochasticOptimizer):\n",
    "    \"\"\"\n",
    "    Fedlin (local SGD with variance control and linear convergence).\n",
    "    \n",
    "    Arguments:\n",
    "        lr (float, optional): an estimate of the inverse smoothness constant\n",
    "        lr_decay_coef (float, optional): the coefficient in front of the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value\n",
    "            is mu/2, where mu is the strong convexity constant\n",
    "        lr_decay_power (float, optional): the power to exponentiate the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value is 1 (default: 1)\n",
    "        it_start_decay (int, optional): how many iterations the step-size is kept constant\n",
    "            By default, will be set to have about 2.5% of iterations with the step-size equal to lr0\n",
    "        batch_size (int, optional): the number of samples from the function to be used at each iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, it_local, n_workers=None, iid=False, lr0=None, lr_max=np.inf, \n",
    "                 lr_decay_coef=0, lr_decay_power=1, it_start_decay=None,\n",
    "                 batch_size=1, losses=None, global_lr=1., cohort_size=None, *args, **kwargs):\n",
    "        super(Fedlin, self).__init__(*args, **kwargs)\n",
    "        self.it_local = it_local\n",
    "        if n_workers is None:\n",
    "            n_workers = psutil.cpu_count(logical=False)\n",
    "        if cohort_size is None:\n",
    "            cohort_size = n_workers\n",
    "        self.n_workers = n_workers\n",
    "        self.cohort_size = cohort_size\n",
    "        self.iid = iid\n",
    "        self.lr0 = lr0\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_decay_coef = lr_decay_coef\n",
    "        self.lr_decay_power = lr_decay_power\n",
    "        self.it_start_decay = it_start_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.losses = losses\n",
    "        self.global_lr = global_lr\n",
    "        \n",
    "    def step(self):\n",
    "        denom_const = 1 / self.lr0\n",
    "        lr_decayed = 1 / (denom_const + self.it_local*self.lr_decay_coef*max(0, self.it-self.it_start_decay)**self.lr_decay_power)\n",
    "        if lr_decayed < 0:\n",
    "            lr_decayed = np.inf \n",
    "        self.lr = min(lr_decayed, self.lr_max)\n",
    "        x_id = ray.put(self.x)\n",
    "        if self.cohort_size != self.n_workers:\n",
    "            raise ValueError(\"There is no theory for FedLin with partial participation. This feature is not implemented.\")\n",
    "            cohort = np.random.choice(self.n_workers, self.cohort_size, replace=False)\n",
    "        g = np.mean(ray.get([worker.get_fedlin_grad.remote(x_id) for worker in self.workers]), axis=0)\n",
    "        g_id = ray.put(g)\n",
    "        if self.cohort_size == self.n_workers:\n",
    "            self.x = np.mean(ray.get([worker.run_fedlin.remote(x_id, self.lr, g_id) for worker in self.workers]), axis=0)\n",
    "        else:\n",
    "            self.x = np.mean(ray.get([self.workers[i].run_fedlin.remote(x_id, self.lr, c_id) for i in cohort]), axis=0)\n",
    "    \n",
    "    def init_run(self, *args, **kwargs):\n",
    "        super(Fedlin, self).init_run(*args, **kwargs)\n",
    "        if self.it_start_decay is None and np.isfinite(self.it_max):\n",
    "            self.it_start_decay = self.it_max // 40 if np.isfinite(self.it_max) else 0\n",
    "        if self.iid:\n",
    "            loss_id = ray.put(self.loss)\n",
    "            self.workers = [Worker.remote(shuffle=False, loss=loss_id, it_local=self.it_local, batch_size=self.batch_size) for _ in range(self.n_workers)]\n",
    "        else:\n",
    "            loss_ids = [ray.put(self.losses[i]) for i in range(self.n_workers)]\n",
    "            self.workers = [Worker.remote(shuffle=False, loss=loss, it_local=self.it_local, batch_size=self.batch_size) for loss in loss_ids]\n",
    "        \n",
    "    def terminate_workers(self):\n",
    "        for worker in self.workers:\n",
    "            ray.kill(worker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yph9Hgl3zcId"
   },
   "source": [
    "# Non-iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Qgtv3nsH53_S"
   },
   "outputs": [],
   "source": [
    "cohort_size = 18 # CHANGE IF YOU WANT PARTIAL PARTICIPATION\n",
    "n_workers = 20\n",
    "n_seeds = 1\n",
    "batch_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVFPQNTAzbKb",
    "outputId": "3194691d-0d16-43cb-967d-eca0e26aa039"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/optmethods/loss/logistic_regression.py:44: UserWarning: The labels have only one unique value.\n",
      "  warnings.warn('The labels have only one unique value.')\n"
     ]
    }
   ],
   "source": [
    "# permutation = A[:, 0].A.squeeze().argsort()\n",
    "permutation = b.squeeze().argsort()\n",
    "\n",
    "losses = []\n",
    "idx = [0] + [(n * i) // n_workers for i in range(1, n_workers)] + [n]\n",
    "for i in range(n_workers):\n",
    "    idx_i = permutation[idx[i] : idx[i+1]]\n",
    "    # idx_i = range(idx[i], idx[i + 1])\n",
    "    loss_i = LogisticRegression(A[idx_i].A, b[idx_i], l1=0, l2=l2)\n",
    "    loss_i.computed_grads = 0\n",
    "    losses.append(loss_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nKWkvnATzu_D"
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(self, shuffle=True, prox_skip=False, loss=None, it_local=None, batch_size=1):\n",
    "        self.loss = loss\n",
    "        self.shuffle = shuffle\n",
    "        self.prox_skip = prox_skip\n",
    "        self.it_local = it_local\n",
    "        self.batch_size = batch_size\n",
    "        self.c = None\n",
    "        self.h = None\n",
    "        self.rng_skip = np.random.default_rng(42) # random number generator for random synchronizations\n",
    "    \n",
    "    def run_local(self, x, lr):\n",
    "        self.x = x * 1.\n",
    "        if self.prox_skip:\n",
    "            self.run_prox_skip(lr)\n",
    "        else:\n",
    "            self.run_local_sgd(lr)\n",
    "        return self.x\n",
    "    \n",
    "    def run_local_sgd(self, lr):\n",
    "        for i in range(self.it_local):\n",
    "            if self.batch_size is None:\n",
    "                self.x -= lr * self.loss.gradient(self.x)\n",
    "            else:\n",
    "                self.x -= lr * self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "    \n",
    "    def run_scaffold(self, x, lr, c):\n",
    "        # as in the original scaffold paper, we use their Option II\n",
    "        self.x = x * 1.\n",
    "        if self.c is None:\n",
    "            self.c = self.x * 0. #initialize zero vector of the same dimension\n",
    "        for i in range(self.it_local):\n",
    "            if self.batch_size is None:\n",
    "                g = self.loss.gradient(self.x)\n",
    "            else:\n",
    "                g = self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "            self.x -= lr * (g - self.c + c)\n",
    "        self.c += 1 / (self.it_local * lr) * (x - self.x) - c\n",
    "        return self.x\n",
    "\n",
    "    def run_prox_skip(self, lr):\n",
    "        p = 1 / self.it_local\n",
    "        if self.h is None:\n",
    "            # first iteration\n",
    "            self.h = self.x * 0. # initialize zero vector of the same dimension\n",
    "        else:\n",
    "            # update the gradient estimate \n",
    "            self.h += p / lr * (self.x - self.x_before_averaing)\n",
    "        it_local = self.rng_skip.geometric(p=p) # since all workers use the same random seed, this number is the same for all of them\n",
    "\n",
    "        for i in range(it_local):\n",
    "            if self.batch_size is None:\n",
    "                g = self.loss.gradient(self.x)\n",
    "            else:\n",
    "                g = self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "            self.x -= lr * (g - self.h)\n",
    "        self.x_before_averaing = self.x * 1.\n",
    "\n",
    "    def run_fedlin(self, x, lr, g):\n",
    "        self.x = x * 1.\n",
    "        for i in range(self.it_local):\n",
    "            if self.batch_size is None:\n",
    "                grad = self.loss.gradient(self.x)\n",
    "            else:\n",
    "                grad = self.loss.stochastic_gradient(self.x, batch_size=self.batch_size)\n",
    "            self.x -= lr * (grad - self.g + g)\n",
    "        return self.x\n",
    "    \n",
    "    def get_control_var(self):\n",
    "        return self.c\n",
    "    \n",
    "    def get_fedlin_grad(self, x):\n",
    "        if self.batch_size is None:\n",
    "            self.g = self.loss.gradient(x)\n",
    "        else:\n",
    "            self.g = self.loss.stochastic_gradient(x, batch_size=self.batch_size)\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ht76OAyxO6uw"
   },
   "outputs": [],
   "source": [
    "class ProxSkip(StochasticOptimizer):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent with decreasing or constant learning rate.\n",
    "    \n",
    "    Arguments:\n",
    "        lr (float, optional): an estimate of the inverse smoothness constant\n",
    "        lr_decay_coef (float, optional): the coefficient in front of the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value\n",
    "            is mu/2, where mu is the strong convexity constant\n",
    "        lr_decay_power (float, optional): the power to exponentiate the number of finished iterations\n",
    "            in the denominator of step-size. For strongly convex problems, a good value is 1 (default: 1)\n",
    "        it_start_decay (int, optional): how many iterations the step-size is kept constant\n",
    "            By default, will be set to have about 2.5% of iterations with the step-size equal to lr0\n",
    "        batch_size (int, optional): the number of samples from the function to be used at each iteration\n",
    "    \"\"\"\n",
    "    def __init__(self, it_local, n_workers=None, cohort_size=None, iid=False, lr0=None, lr_max=np.inf, lr_decay_coef=0,\n",
    "                 lr_decay_power=1, it_start_decay=None, batch_size=1, losses=None, *args, **kwargs):\n",
    "        super(ProxSkip, self).__init__(*args, **kwargs)\n",
    "        self.it_local = it_local\n",
    "        if n_workers is None:\n",
    "            n_workers = psutil.cpu_count(logical=False)\n",
    "        if cohort_size is None:\n",
    "            cohort_size = n_workers\n",
    "        self.n_workers = n_workers\n",
    "        self.cohort_size = cohort_size\n",
    "        self.iid = iid\n",
    "        self.lr0 = lr0\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_decay_coef = lr_decay_coef\n",
    "        self.lr_decay_power = lr_decay_power\n",
    "        self.it_start_decay = it_start_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.losses = losses\n",
    "        \n",
    "    def step(self):\n",
    "        denom_const = 1 / self.lr0\n",
    "        lr_decayed = 1 / (denom_const + self.it_local*self.lr_decay_coef*max(0, self.it-self.it_start_decay)**self.lr_decay_power)\n",
    "        if lr_decayed < 0:\n",
    "            lr_decayed = np.inf\n",
    "        self.lr = min(lr_decayed, self.lr_max)\n",
    "        x_id = ray.put(self.x)\n",
    "        \n",
    "        if self.cohort_size == self.n_workers:\n",
    "            self.x = np.mean(ray.get([worker.run_local.remote(x_id, self.lr) for worker in self.workers]), axis=0)\n",
    "        else:\n",
    "            cohort = np.random.choice(self.n_workers, self.cohort_size, replace=False)\n",
    "            self.x = np.mean(ray.get([self.workers[i].run_local.remote(x_id, self.lr) for i in cohort]), axis=0)\n",
    "    \n",
    "    def init_run(self, *args, **kwargs):\n",
    "        super(ProxSkip, self).init_run(*args, **kwargs)\n",
    "        if self.it_start_decay is None and np.isfinite(self.it_max):\n",
    "            self.it_start_decay = self.it_max // 40 if np.isfinite(self.it_max) else 0\n",
    "        if self.lr0 is None:\n",
    "            self.lr0 = 1 / self.loss.batch_smoothness(batch_size)\n",
    "        if self.iid:\n",
    "            loss_id = ray.put(self.loss)\n",
    "            self.workers = [Worker.remote(shuffle=False, prox_skip=True, loss=loss_id, it_local=self.it_local, batch_size=self.batch_size) for _ in range(self.n_workers)]\n",
    "        else:\n",
    "            loss_ids = [ray.put(self.losses[i]) for i in range(self.n_workers)]\n",
    "            self.workers = [Worker.remote(shuffle=False, prox_skip=True, loss=loss, it_local=self.it_local, batch_size=self.batch_size) for loss in loss_ids]\n",
    "        \n",
    "    def update_trace(self, first_iterations=10):\n",
    "        super(ProxSkip, self).update_trace()\n",
    "        \n",
    "    def terminate_workers(self):\n",
    "        for worker in self.workers:\n",
    "            ray.kill(worker)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "fed-learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
